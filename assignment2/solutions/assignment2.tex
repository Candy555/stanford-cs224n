\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{float}

\setlength{\parindent}{0pt}

\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}

\title{ CS 224n: Assignment \#2 }
\author{Kristian Hartikainen}
\date{\today}
\pagestyle{myheadings}

\newcommand{\R}{\mathbb{R}}

\begin{document}
\maketitle

\section{Tensorflow Softmax}
\subsection*{(c)}
\textbf{Question:} Carefully study the Model class in model.py. Briefly explain the purpose
of placeholder variables and feed dictionaries in TensorFlow computations.

\textbf{Answer:} The placeholder variables in TensorFlow are used to present variables in computation graph that expect raw data to be fed in them when the computation graph is evaluated. Feed dictionaries are used to pass these raw data to the computation graph at the time of evaluation.

\subsection*{(e)}
\textbf{Question:} Explain how TensorFlow’s automatic differentiation removes the need for us to define gradients explicitly.

\textbf{Answer:} Each TensorFlow operator defines methods needed to compute the gradient with respect to that operators using chain rule. One method calculates the forward pass through that operator and saves the values needed in the backward pass in cache. The backward pass method uses the cache to propagate the gradients through the operator using the gradient from the downstream operator.

\section{Neural Transition-Based Dependency Parsing}
\subsection*{(a)}
\textbf{Question:} Go through the sequence of transitions needed for parsing the sentence “I parsed this sentence correctly”.

\textbf{Answer:}
\begin{table}[H]
  \centering
  \begin{tabular}{p{\linewidth/3}|p{\linewidth/3}|p{\linewidth/6}|p{\linewidth/6}}
    stack  & buffer & new dependency & transition\\
    \hline
    [ROOT]                         & [I, parsed, this, sentence, correctly] &                         & Initial Configuration \\\relax
    [ROOT, I]                      & [parsed, this, sentence, correctly]    &                         & \verb|SHIFT| \\\relax
    [ROOT, I, parsed]              & [this, sentence, correctly]            &                         & \verb|SHIFT| \\\relax
    [ROOT, parsed]                 & [this, sentence, correctly]            & parsed $\to$ I          & \verb|LEFT-ARC| \\\relax
    [ROOT, parsed, this]           & [sentence, correctly]                  &                         & \verb|SHIFT| \\\relax
    [ROOT, parsed, this, sentence] & [correctly]                            &                         & \verb|SHIFT| \\\relax
    [ROOT, parsed, sentence]       & [correctly]                            & sentence $\to$ this     & \verb|LEFT-ARC| \\\relax
    [ROOT, parsed]                 & [correctly]                            & parsed $\to$ sentence   & \verb|RIGHT-ARC| \\\relax
    [ROOT, parsed, correctly]      & []                                     &                         & \verb|SHIFT| \\\relax
    [ROOT, parsed]                 & []                                     & parsed $\to$ correctly  & \verb|RIGHT-ARC| \\\relax
    [ROOT]                         & []                                     & ROOT $\to$ parsed       & \verb|RIGHT-ARC| \\\relax
  \end{tabular}
\end{table}

\subsection*{(b)}

\textbf{Question:} A sentence containing n words will be parsed in how many steps (in terms of n)? Briefly explain why.

\textbf{Answer:} During parsing, each word moves at some point (once) from buffer to stack, and is eventually (once) removed from the stack. Thus, for n words, we need n \verb|SHIFT|s and n \verb|ARC|s. On each move, we either \verb|SHIFT| or \verb|ARC| $\to$ 2n steps needed.

\subsection*{(f)}
\textbf{Question:} We will regularize our network by applying Dropout. During training this randomly sets units in the hidden layer \textbf{h} to zero with probability $p_{drop}$ and then multiplies \textbf{h} by a constant $\gamma$ (dropping different units each minibatch). We can write this as \[\mathbf{h}_{drop} = \gamma \mathbf{d} \circ \mathbf{h}\] where $\mathbf{d} \in {\{0, 1\}}^{D_h}$ ($D_h$ is the size of \textbf{h}) is a mask vector where each entry is 0 with probability $p_{drop}$ and 1 with probability ($1 - p_{drop}$). $\gamma$ is chosen such that the value of $\mathbf{h}_{drop}$ in expectation equals h: \[E_{p_{drop}} [\mathbf{h}_{drop}]_i = \mathbf{h}_i\]
for all $0 < i < Dh$. What must γ equal in terms of $p_{drop}$? Briefly justify your answer.

\textbf{Answer:}
\begin{equation*}
  \begin{split}
    E_{p_{drop}} [\mathbf{h}_{drop}]_i = \mathbf{h}_i   &= \mathbf{h}_i \\
    E_{p_{drop}} [\gamma \mathbf{d} \circ \mathbf{h}]_i &= \mathbf{h}_i \\
    E_{p_{drop}} [\mathbf{d}_i \mathbf{h}_i] \gamma \mathbf{h}_i &= \mathbf{h}_i  \\
    \gamma &= \frac{ \mathbf{h}_i }{ E_{p_{drop}} [\mathbf{d}_i] \mathbf{h}_i }  \\
    &= \frac{ 1 }{ E_{p_{drop}} [\mathbf{d}_i] } \\
    &= \frac{ 1 }{ (0p_{drop} + 1(1-p_{drop})) } \\
    &= \frac{ 1 }{ 1-p_{drop} }
  \end{split}
\end{equation*}

\section{Recurrent Neural Networks: Language Modeling}
\subsection*{(a)}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
