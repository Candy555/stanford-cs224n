\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{mathtools}

\setlength{\parindent}{0pt}

\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}

\title{ CS 224n: Assignment \#2 }
\author{Kristian Hartikainen}
\date{\today}
\pagestyle{myheadings}

\newcommand{\R}{\mathbb{R}}

\begin{document}
\maketitle

\section{Tensorflow Softmax}
\subsection*{(c)}
\textbf{Question:} Carefully study the Model class in model.py. Briefly explain the purpose
of placeholder variables and feed dictionaries in TensorFlow computations.

\textbf{Answer:} The placeholder variables in TensorFlow are used to present variables in computation graph that expect raw data to be fed in them when the computation graph is evaluated. Feed dictionaries are used to pass these raw data to the computation graph at the time of evaluation.

\subsection*{(e)}
\textbf{Question:} Explain how TensorFlowâ€™s automatic differentiation removes the need for us to define gradients explicitly.

\textbf{Answer:} Each TensorFlow operator defines methods needed to compute the gradient with respect to that operators using chain rule. One method calculates the forward pass through that operator and saves the values needed in the backward pass in cache. The backward pass method uses the cache to propagate the gradients through the operator using the gradient from the downstream operator.

\section{Neural Transition-Based Dependency Parsing}
\subsection*{(a)}

\section{Recurrent Neural Networks: Language Modeling}
\subsection*{(a)}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
